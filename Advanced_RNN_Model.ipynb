{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1540rRZ3tff"
      },
      "source": [
        "#Advanced RNN Model\n",
        "\n",
        "In the Simple RNN Model, we observed very poor performance. We will now try to improve that performance by using the following:\n",
        "- packed padded sequences\n",
        "- pre-trained word embeddings\n",
        "- different RNN architecture\n",
        "- bidirectional RNN\n",
        "- multi-layer RNN\n",
        "- regularization\n",
        "- a different optimizer\n",
        "\n",
        "Making this enhancements helps us to achieve ~84% test accuracy. It can be ran by changing the runtime type to \"GPU\" and selecting \"run all\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lwvMrjy5J3sw",
        "outputId": "a01e4d57-7a65-4e83-d1ef-071be9e1b476"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# testing for GPU\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6kbhGRO3tfh"
      },
      "source": [
        "## Preparing the Data\n",
        "\n",
        "Firstly, we set the seed, define the `Fields`, and retrieve the train/val/test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzR35c4R4zSw",
        "outputId": "0caeefef-5664-4fd8-d95d-c1a663afc42e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.6\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchtext==0.6) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchtext==0.6) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from torchtext==0.6) (2.0.0+cu118)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from torchtext==0.6) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from torchtext==0.6) (4.65.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.6) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.6) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.6) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchtext==0.6) (1.26.15)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (4.5.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->torchtext==0.6) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchtext==0.6) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch->torchtext==0.6) (3.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->torchtext==0.6) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->torchtext==0.6) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.1\n",
            "    Uninstalling torchtext-0.15.1:\n",
            "      Successfully uninstalled torchtext-0.15.1\n",
            "Successfully installed sentencepiece-0.1.98 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be using *packed padded sequences* by setting `include_lengths = True` for our `TEXT` field which will make our RNN only process the non-padded elements of our sequence, and for any padded element the `output` will be a zero tensor.  This will cause `batch.text` to now be a tuple with the first element being our sentence (a numericalized tensor that has been padded) and the second element being the actual lengths of our sentences."
      ],
      "metadata": {
        "id": "PbcHDjvLSizM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzxmqxRw3tfh",
        "outputId": "dd7ba7d5-18c4-4538-9ffd-fbd55bad6db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torchtext.data.field.Field object at 0x7f82c3d52370>\n",
            "<torchtext.data.field.LabelField object at 0x7f835fcbf520>\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize = 'spacy',\n",
        "                  tokenizer_language = 'en_core_web_sm',\n",
        "                  include_lengths = True)\n",
        "\n",
        "LABEL = data.LabelField(dtype = torch.float)\n",
        "\n",
        "print(TEXT)\n",
        "print(LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmhRA-gb3tfi"
      },
      "source": [
        "Load the IMDb dataset and split into train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c03PHRCV3tfj",
        "outputId": "1cd704e8-6bf5-4a1c-a85e-16a72e4d118f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:07<00:00, 11.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "from torchtext import datasets\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D91SUL73tfj"
      },
      "source": [
        "Then create the validation set from our training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akh_43mE3tfj"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "train_data, valid_data = train_data.split(split_ratio=0.5, random_state = random.seed(SEED))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx-FYCqc3tfj"
      },
      "source": [
        "We indicate that we want to use pre-trained word embeddings by passing `\"glove.6B.100d\" vectors\"` as an argument to `build_vocab`. `glove` is the algorithm used to calculate the vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GxI0alJ3tfk",
        "outputId": "e6faf83c-3ce8-4b13-dc89-7136b095f417"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.36MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:19<00:00, 20604.59it/s]\n"
          ]
        }
      ],
      "source": [
        "MAX_VOCAB_SIZE = 25_000\n",
        "\n",
        "TEXT.build_vocab(train_data, \n",
        "                 max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = \"glove.6B.100d\", \n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "LABEL.build_vocab(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2AnSzwk3tfk"
      },
      "source": [
        "## Batch Sizes\n",
        "In the below cell, we test different batch sizes of 32, 64, and 128 for the iterator. Additionally, we ensure that all the packed padded sequences tensors are sorted by their lengths by setting `sort_within_batch = True` in the iterator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VEu5uUF3tfk"
      },
      "outputs": [],
      "source": [
        "# We can adjust the batch size here\n",
        "BATCH_SIZE = 64\n",
        "# BATCH_SIZE = 32\n",
        "# BATCH_SIZE = 128\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_within_batch = True,\n",
        "    device = device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VguqRkH3tfl"
      },
      "source": [
        "## Build the Model\n",
        "#### Different RNN Architecture\n",
        "We used an entirely different RNN architecture (LSTM) so that we can overcome the vanishing gradient problem RNNs have. \n",
        "\n",
        "#### Bidirectional RNN\n",
        "As well as having an RNN processing the words in the sentence from the first to the last (a forward RNN), we have a second RNN processing the words in the sentence from the **last to the first** (a backward RNN). At time step $t$, the forward RNN is processing word $x_t$, and the backward RNN is processing word $x_{T-t+1}$. \n",
        "\n",
        "#### Multi-layer RNN\n",
        "We also utilized a multi-layer RNN (also known as Deep RNNs) by adding additional RNNs on top of the initial standard RNN where each added RNN is a new layer. \n",
        "\n",
        "#### Regularization\n",
        "\n",
        "To combat poor generalization and overfitting, we use a method of regularization called dropout. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFZ5uHMw3tfl"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
        "                 bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        self.rnn = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout=dropout) \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):     \n",
        "        embedded = self.dropout(self.embedding(text))      \n",
        "        # pack sequence and ensure lengths are on CPU\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        #unpack sequence\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))           \n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we create an instance of our RNN class with our new parameters and arguments for the number of layers, bidirectionality, and dropout probability."
      ],
      "metadata": {
        "id": "_Z7yGHb11Tav"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBroyegr3tfl"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 2\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5 # test with 0.6, 0.8\n",
        "\n",
        "# retrieving pad token index from vocabulary in string format\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "model = RNN(INPUT_DIM, \n",
        "            EMBEDDING_DIM, \n",
        "            HIDDEN_DIM, \n",
        "            OUTPUT_DIM, \n",
        "            N_LAYERS, \n",
        "            BIDIRECTIONAL, \n",
        "            DROPOUT, \n",
        "            PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFAOaVKc3tfm",
        "outputId": "dbb92ac7-6f5a-4a0d-9372-442fbc97a158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 4,810,857 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "# printing the number of parameters in our model \n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT0pWwyP3tfm",
        "outputId": "362d1831-0236-4eb9-e4bf-8db1a800147a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([25002, 100])\n"
          ]
        }
      ],
      "source": [
        "# copying the pre-trained word embeddings into embedding layer of our model.\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "# print shape to check if embeddings are the correct size, [vocab size, embedding dimension]\n",
        "print(pretrained_embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOqgqhMY3tfn"
      },
      "source": [
        "We then replace the initial weights of the `embedding` layer with the pre-trained embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383TgfjM3tfn",
        "outputId": "a164afc1-b8f4-429d-c012-d0274fdd808c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],\n",
              "        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.4514,  0.2532, -0.4848,  ..., -0.8656,  0.0834,  0.3125],\n",
              "        [-2.1498,  0.0503, -2.1136,  ...,  0.7646, -0.3180, -0.1118],\n",
              "        [-0.6409,  1.7305,  1.1259,  ...,  0.0879,  0.1361,  0.4924]])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G64RXLVm3tfn",
        "outputId": "c2ae3eef-6ae8-4f86-ca35-5c7ba656c579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.4514,  0.2532, -0.4848,  ..., -0.8656,  0.0834,  0.3125],\n",
            "        [-2.1498,  0.0503, -2.1136,  ...,  0.7646, -0.3180, -0.1118],\n",
            "        [-0.6409,  1.7305,  1.1259,  ...,  0.0879,  0.1361,  0.4924]])\n"
          ]
        }
      ],
      "source": [
        "# initializing both <unk> and <pad> token to all zeros to explicity tell model that they are irrelevant.\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opuO0Kd73tfn"
      },
      "source": [
        "## Training and Choosing Optimizer and Criterion for the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeUmPE4l3tfn"
      },
      "source": [
        "Now we train the model. We choose Adam for the optimizer and use a BCEWithLogitsLoss function for the criterion and place the model on a GPU if available. Additionally, we experiment with another optimizer, SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GYZ9OVL3tfn"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# trying SGD\n",
        "# optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
        "\n",
        "# trying Adam\n",
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXv2POE23tfn"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97avfs2Q3tfn"
      },
      "source": [
        "We implement the function to calculate accuracy..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLCTkDWW3tfn"
      },
      "outputs": [],
      "source": [
        "def binary_accuracy(preds, y):\n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "\n",
        "    #round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_RypF5HrqXg"
      },
      "source": [
        "We implement the function to calculate precision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97MsDio-rzf4"
      },
      "outputs": [],
      "source": [
        "def binary_precision(preds, y):\n",
        "    \"\"\"\n",
        "    Returns precision per batch, i.e. if you get 8 true positives/10 true positives + false positives right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    # correct = (rounded_preds == y).float() #convert into float for division \n",
        "    true_pos = 0\n",
        "    rounded_preds.tolist()\n",
        "    y.tolist()\n",
        "    for idx, x in enumerate(rounded_preds):\n",
        "      if x == 1 and y[idx] == 1:\n",
        "        true_pos += 1\n",
        "\n",
        "    # true + false positives\n",
        "    pred_ones = (rounded_preds == 1.).sum(dim=0)\n",
        "\n",
        "    # true + false positives\n",
        "    precision = true_pos/ (pred_ones+1e-8)\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL2Z_btnx2xr"
      },
      "source": [
        "We implement the function to calculate recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1EyyJ0iUx2hp"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "\n",
        "def binary_recall(preds, y):\n",
        "    \"\"\"\n",
        "    Returns recall per batch, i.e. if you get 8 true positives/10 true positives + false negatives right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    # round predictions to the closest integer\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "\n",
        "    true_pos = 0\n",
        "    false_neg = 0\n",
        "\n",
        "    rounded_preds.tolist()\n",
        "    y.tolist()\n",
        "    for idx, x in enumerate(rounded_preds):\n",
        "      if x == 1 and y[idx] == 1:\n",
        "        true_pos += 1\n",
        "      if x == 0 and y[idx] == 1:\n",
        "        false_neg += 1\n",
        "\n",
        "    # true positives + false neg\n",
        "    t_pos_f_neg = (y == 1.).sum(dim=0)\n",
        "    # t_pos_f_neg = true_pos + false_neg \n",
        "\n",
        "    # print(\"True Positives: \" + str(true_pos))\n",
        "    # print(\"True Positives + False Negatives: \" + str(t_pos_f_neg))\n",
        "    recall = true_pos/ (t_pos_f_neg+1e-8)\n",
        "    # print(\"Recall: \" + str(recall))\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da3B928jzMZr"
      },
      "source": [
        "We implement the function calculate F1 Score. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teF4ZfFZzMKS"
      },
      "outputs": [],
      "source": [
        "def binary_f1(prec, recall):\n",
        "    \"\"\"\n",
        "    Returns f1 per batch\n",
        "    \"\"\"\n",
        "    f1 = 2 * prec * recall / ((prec + recall) + 1e-8)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LgXKAyXU3tfn"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_prec = 0\n",
        "    epoch_rec = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "        optimizer.zero_grad()        \n",
        "        text, text_lengths = batch.text        \n",
        "        predictions = model(text, text_lengths).squeeze(1)        \n",
        "        loss = criterion(predictions, batch.label)        \n",
        "        acc = binary_accuracy(predictions, batch.label)\n",
        "        prec = binary_precision(predictions, batch.label)\n",
        "        rec = binary_recall(predictions, batch.label)       \n",
        "        loss.backward()        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_prec += prec.item()\n",
        "        epoch_rec += rec.item()\n",
        "       \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_prec / len(iterator), epoch_rec / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQb0fE123tfo"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_prec = 0\n",
        "    epoch_rec = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "            text, text_lengths = batch.text           \n",
        "            predictions = model(text, text_lengths).squeeze(1)            \n",
        "            loss = criterion(predictions, batch.label)           \n",
        "            acc = binary_accuracy(predictions, batch.label)\n",
        "            prec = binary_precision(predictions, batch.label)\n",
        "            rec = binary_recall(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            epoch_prec += prec.item()\n",
        "            epoch_rec += rec.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator), epoch_prec / len(iterator), epoch_rec / len(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqOYZScP3tfo"
      },
      "source": [
        "We define a function to inform us how long each epoch takes to run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZxC30yp3tfo"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ7ldOi93tfo"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUr0lA_q3tfo",
        "outputId": "38385713-989d-49ab-fefb-8cf7c862e806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: -78.023 | Train Acc: 50.72 | Train Prec: 0.00 | Train Recall: 0.00 | Train F1: 0.00%\n",
            "\t Val. Loss: -105.602 |  Val. Acc: 49.26 | Val. Prec: 0.00 | Val Recall: 0.00 | Val F1: 0.00%\n",
            "Epoch: 02 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: -126.359 | Train Acc: 50.74 | Train Prec: 0.00 | Train Recall: 0.00 | Train F1: 0.00%\n",
            "\t Val. Loss: -155.292 |  Val. Acc: 49.26 | Val. Prec: 0.00 | Val Recall: 0.00 | Val F1: 0.00%\n",
            "Epoch: 03 | Epoch Time: 0m 29s\n",
            "\tTrain Loss: -175.204 | Train Acc: 50.65 | Train Prec: 0.00 | Train Recall: 0.00 | Train F1: 0.00%\n",
            "\t Val. Loss: -205.341 |  Val. Acc: 49.26 | Val. Prec: 0.00 | Val Recall: 0.00 | Val F1: 0.00%\n",
            "Epoch: 04 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: -223.781 | Train Acc: 50.72 | Train Prec: 0.00 | Train Recall: 0.00 | Train F1: 0.00%\n",
            "\t Val. Loss: -255.407 |  Val. Acc: 49.26 | Val. Prec: 0.00 | Val Recall: 0.00 | Val F1: 0.00%\n",
            "Epoch: 05 | Epoch Time: 0m 30s\n",
            "\tTrain Loss: -271.890 | Train Acc: 50.76 | Train Prec: 0.00 | Train Recall: 0.00 | Train F1: 0.00%\n",
            "\t Val. Loss: -305.300 |  Val. Acc: 49.26 | Val. Prec: 0.00 | Val Recall: 0.00 | Val F1: 0.00%\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    start_time = time.time()  \n",
        "    train_loss, train_acc, train_prec, train_rec = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc, valid_prec, valid_rec = evaluate(model, valid_iterator, criterion)   \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "\n",
        "    train_f1 = binary_f1(train_prec, train_rec)\n",
        "    valid_f1 = binary_f1(valid_prec, valid_rec)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train Prec: {train_prec*100:.2f} | Train Recall: {train_rec*100:.2f} | Train F1: {train_f1*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} | Val. Prec: {valid_prec*100:.2f} | Val Recall: {valid_rec*100:.2f} | Val F1: {valid_f1*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlpiLjzw3tfo"
      },
      "source": [
        "### Obtain Test Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yXVcphX3tfo"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('tut2-model.pt'))\n",
        "\n",
        "test_loss, test_acc, test_prec, test_recall = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "test_f1 = binary_f1(test_prec, test_recall)\n",
        "\n",
        "# print(test_recall)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f}% | Test Acc: {test_acc*100:.2f}% | Test Precision: {test_prec*100:.2f}% | Test Recall: {test_recall*100:.2f}% | Test F1 Score: {test_f1*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}